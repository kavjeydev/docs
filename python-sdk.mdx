---
title: "Python SDK"
description: "Complete guide to the Trainly Python SDK"
---

## Installation

Install the Trainly SDK using pip or poetry:

<CodeGroup>

```bash pip
pip install trainly
```

```bash poetry
poetry add trainly
```

```bash pipenv
pipenv install trainly
```

</CodeGroup>

## Quick Start

```python
from trainly import TrainlyClient

# Initialize the client
trainly = TrainlyClient(
    api_key="tk_your_api_key",
    chat_id="chat_abc123"
)

# Ask a question
response = trainly.query(
    question="What are the main findings?"
)

print("Answer:", response.answer)
print("Citations:", len(response.context))
```

## Initialization

### Basic Configuration

```python
from trainly import TrainlyClient
import os

trainly = TrainlyClient(
    api_key=os.getenv("TRAINLY_API_KEY"),
    chat_id=os.getenv("TRAINLY_CHAT_ID")
)
```

### Advanced Configuration

```python
from trainly import TrainlyClient
import logging

trainly = TrainlyClient(
    api_key="tk_your_api_key",
    chat_id="chat_abc123",

    # API Configuration
    base_url="https://api.trainlyai.com",
    timeout=30.0,  # Request timeout in seconds

    # Retry Configuration
    max_retries=3,
    retry_delay=1.0,  # Initial delay in seconds
    retry_backoff=2.0,  # Exponential backoff multiplier

    # Default Query Options
    default_model="gpt-4o-mini",
    default_temperature=0.7,
    default_max_tokens=1000,

    # Logging
    debug=False,
    logger=logging.getLogger(__name__)
)
```

## Querying Documents

### Basic Query

```python
from trainly import TrainlyClient

trainly = TrainlyClient(
    api_key="tk_your_api_key",
    chat_id="chat_abc123"
)

# Ask a question
response = trainly.query(
    question="What is the main conclusion?"
)

# Access response fields
print("Answer:", response.answer)
print("Model:", response.model)
print("Tokens used:", response.usage.total_tokens)

# Access citations
for i, chunk in enumerate(response.context):
    print(f"[{i}] {chunk.chunk_text[:100]}...")
    print(f"    Relevance: {chunk.score * 100:.1f}%")
```

### Advanced Query Options

```python
response = trainly.query(
    question="Explain the methodology in detail",

    # Model Selection
    model="gpt-4o",  # or 'claude-3-opus', 'gpt-4-turbo', etc.
    temperature=0.3,  # Lower = focused, Higher = creative
    max_tokens=2000,  # Maximum response length

    # Custom Instructions
    custom_prompt="You are a research analyst. Provide detailed technical explanations with citations.",

    # Scope Filtering
    scope_filters={
        "project_id": "proj_123",
        "version": "2.0",
        "category": "research"
    }
)

print(response.answer)
```

### Type Hints

```python
from trainly import TrainlyClient, QueryResponse, ChunkScore
from typing import List, Dict, Any

trainly = TrainlyClient(
    api_key="tk_your_api_key",
    chat_id="chat_abc123"
)

# Fully typed response
response: QueryResponse = trainly.query(
    question="What are the results?",
    model="gpt-4o-mini",
    temperature=0.7
)

# Access typed fields
answer: str = response.answer
context: List[ChunkScore] = response.context
tokens: int = response.usage.total_tokens

# Iterate with type hints
chunk: ChunkScore
for chunk in response.context:
    chunk_id: str = chunk.chunk_id
    text: str = chunk.chunk_text
    score: float = chunk.score
```

## Streaming Responses

### Basic Streaming

```python
# Stream responses in real-time
for chunk in trainly.query_stream(
    question="Summarize all the key points"
):
    if chunk.type == "content":
        print(chunk.data, end="", flush=True)
    elif chunk.type == "context":
        print(f"\n\nUsing {len(chunk.data)} citations")
    elif chunk.type == "end":
        print("\n\nComplete!")
```

### Advanced Streaming with Progress

```python
from trainly import TrainlyClient
from rich.console import Console
from rich.progress import Progress, SpinnerColumn, TextColumn

console = Console()
trainly = TrainlyClient(
    api_key="tk_your_api_key",
    chat_id="chat_abc123"
)

def stream_query(question: str):
    """Stream query with fancy progress display"""

    with Progress(
        SpinnerColumn(),
        TextColumn("[progress.description]{task.description}"),
        console=console
    ) as progress:
        task = progress.add_task("Generating response...", total=None)

        answer = ""

        for chunk in trainly.query_stream(question=question):
            if chunk.type == "content":
                answer += chunk.data
                console.print(chunk.data, end="")
            elif chunk.type == "end":
                progress.update(task, description="Complete!")
                break

        return answer

# Usage
answer = stream_query("What are the key findings?")
console.print(f"\n\nFinal answer length: {len(answer)} characters")
```

### Async Streaming

```python
import asyncio
from trainly import AsyncTrainlyClient

async def stream_query_async():
    trainly = AsyncTrainlyClient(
        api_key="tk_your_api_key",
        chat_id="chat_abc123"
    )

    async for chunk in trainly.query_stream(
        question="Explain the results"
    ):
        if chunk.type == "content":
            print(chunk.data, end="", flush=True)
        elif chunk.type == "end":
            print("\n\nDone!")

# Run async function
asyncio.run(stream_query_async())
```

## File Management

### Upload Files

```python
# Upload from file path
with open("research_paper.pdf", "rb") as file:
    result = trainly.upload_file(
        file=file,
        filename="research_paper.pdf",
        scope_values={
            "project_id": "proj_123",
            "category": "research",
            "version": 1.0
        }
    )

print(f"Uploaded: {result.filename}")
print(f"File ID: {result.file_id}")
print(f"Size: {result.size_bytes} bytes")

# Upload from bytes
file_bytes = b"This is my document content..."
result = trainly.upload_file(
    file=file_bytes,
    filename="notes.txt",
    scope_values={"type": "notes"}
)
```

### Upload Text Content

```python
# Upload raw text without a file
result = trainly.upload_text(
    content="This is my document content. It contains important information...",
    name="My Notes.txt",
    scope_values={
        "type": "notes",
        "date": "2024-01-15"
    }
)

print(f"Text uploaded as: {result.filename}")
```

### Bulk Upload

```python
from pathlib import Path
from typing import List

# Find all PDF files
pdf_files = list(Path("./documents").rglob("*.pdf"))

results = trainly.upload_bulk(
    files=[
        {
            "file": open(path, "rb"),
            "filename": path.name,
            "scope_values": {
                "source": "research",
                "upload_date": "2024-01-15"
            }
        }
        for path in pdf_files
    ],
    on_progress=lambda completed, total: print(f"Progress: {completed}/{total}")
)

print(f"Success: {results.successful_uploads}/{results.total_files}")

# Check for failures
for result in results.results:
    if not result.success:
        print(f"Failed: {result.filename} - {result.error}")
```

### Bulk Upload with Progress Bar

```python
from trainly import TrainlyClient
from tqdm import tqdm
from pathlib import Path

trainly = TrainlyClient(
    api_key="tk_your_api_key",
    chat_id="chat_abc123"
)

# Get all PDF files
pdf_files = list(Path("./documents").rglob("*.pdf"))

# Upload with progress bar
results = []
with tqdm(total=len(pdf_files), desc="Uploading files") as pbar:
    for pdf_path in pdf_files:
        try:
            with open(pdf_path, "rb") as file:
                result = trainly.upload_file(
                    file=file,
                    filename=pdf_path.name,
                    scope_values={"source": "batch_upload"}
                )
                results.append({"file": pdf_path.name, "success": True})
        except Exception as e:
            results.append({"file": pdf_path.name, "success": False, "error": str(e)})
        finally:
            pbar.update(1)

# Summary
successful = sum(1 for r in results if r["success"])
print(f"\nUploaded {successful}/{len(results)} files successfully")
```

### Upload from URL

```python
import requests
from io import BytesIO

def upload_from_url(url: str, filename: str):
    """Download file from URL and upload to Trainly"""

    # Download file
    response = requests.get(url, stream=True)
    response.raise_for_status()

    # Get file content
    file_content = BytesIO(response.content)

    # Upload to Trainly
    result = trainly.upload_file(
        file=file_content,
        filename=filename,
        scope_values={
            "source": "url",
            "original_url": url
        }
    )

    return result

# Usage
result = upload_from_url(
    "https://example.com/research_paper.pdf",
    "research_paper.pdf"
)

print(f"Uploaded from URL: {result.filename}")
```

### Upload with Validation

```python
from pathlib import Path
from typing import Optional
import mimetypes

class FileValidator:
    """Validate files before uploading"""

    MAX_SIZE = 5 * 1024 * 1024  # 5MB
    ALLOWED_TYPES = {
        'application/pdf',
        'application/vnd.openxmlformats-officedocument.wordprocessingml.document',
        'text/plain',
        'text/markdown',
        'application/json'
    }

    @classmethod
    def validate(cls, file_path: Path) -> tuple[bool, Optional[str]]:
        """Validate file before upload"""

        # Check if file exists
        if not file_path.exists():
            return False, f"File not found: {file_path}"

        # Check file size
        file_size = file_path.stat().st_size
        if file_size > cls.MAX_SIZE:
            return False, f"File too large: {file_size / 1024 / 1024:.1f}MB (max 5MB)"

        # Check MIME type
        mime_type, _ = mimetypes.guess_type(str(file_path))
        if mime_type not in cls.ALLOWED_TYPES:
            return False, f"Unsupported file type: {mime_type}"

        return True, None

# Usage with validation
def safe_upload(file_path: str):
    """Upload with validation"""
    path = Path(file_path)

    # Validate before upload
    is_valid, error = FileValidator.validate(path)

    if not is_valid:
        print(f"Validation failed: {error}")
        return None

    # Upload if valid
    try:
        with open(path, "rb") as file:
            result = trainly.upload_file(
                file=file,
                filename=path.name
            )

        print(f"✓ Uploaded: {result.filename}")
        return result

    except Exception as e:
        print(f"✗ Upload failed: {e}")
        return None

# Example
result = safe_upload("./documents/research.pdf")
```

### Upload with Checksum Verification

```python
import hashlib
from pathlib import Path

def calculate_md5(file_path: Path) -> str:
    """Calculate MD5 checksum of file"""
    md5_hash = hashlib.md5()

    with open(file_path, "rb") as f:
        for chunk in iter(lambda: f.read(4096), b""):
            md5_hash.update(chunk)

    return md5_hash.hexdigest()

def upload_with_checksum(file_path: str, expected_checksum: str = None):
    """Upload file with checksum verification"""
    path = Path(file_path)

    # Calculate checksum
    actual_checksum = calculate_md5(path)
    print(f"File checksum: {actual_checksum}")

    # Verify if expected checksum provided
    if expected_checksum and actual_checksum != expected_checksum:
        raise ValueError(
            f"Checksum mismatch! Expected {expected_checksum}, got {actual_checksum}"
        )

    # Upload file
    with open(path, "rb") as file:
        result = trainly.upload_file(
            file=file,
            filename=path.name,
            scope_values={
                "checksum": actual_checksum,
                "verified": expected_checksum is not None
            }
        )

    print(f"✓ Uploaded with verified checksum: {result.filename}")
    return result

# Usage
result = upload_with_checksum(
    "./important_document.pdf",
    expected_checksum="abc123def456..."
)
```

### Upload with Retry Logic

```python
import time
from typing import Optional

def upload_with_retry(
    file_path: str,
    max_attempts: int = 3,
    retry_delay: float = 1.0
) -> Optional[any]:
    """Upload file with automatic retry on failure"""

    for attempt in range(1, max_attempts + 1):
        try:
            print(f"Upload attempt {attempt}/{max_attempts}...")

            with open(file_path, "rb") as file:
                result = trainly.upload_file(
                    file=file,
                    filename=Path(file_path).name
                )

            print(f"✓ Upload successful on attempt {attempt}")
            return result

        except Exception as e:
            print(f"✗ Attempt {attempt} failed: {e}")

            if attempt < max_attempts:
                # Exponential backoff
                wait_time = retry_delay * (2 ** (attempt - 1))
                print(f"Waiting {wait_time:.1f}s before retry...")
                time.sleep(wait_time)
            else:
                print(f"Failed after {max_attempts} attempts")
                raise

# Usage
try:
    result = upload_with_retry(
        "./research.pdf",
        max_attempts=3,
        retry_delay=2.0
    )
except Exception as e:
    print(f"Upload permanently failed: {e}")
```

### Upload Directory Structure

```python
from pathlib import Path
from collections import defaultdict

def upload_directory(
    directory: str,
    preserve_structure: bool = True,
    file_extensions: list = None
):
    """Upload all files from a directory"""

    dir_path = Path(directory)

    if not dir_path.is_dir():
        raise ValueError(f"Not a directory: {directory}")

    # Default to common document types
    if file_extensions is None:
        file_extensions = ['.pdf', '.docx', '.txt', '.md']

    # Find all matching files
    files_by_type = defaultdict(list)

    for ext in file_extensions:
        files = list(dir_path.rglob(f"*{ext}"))
        files_by_type[ext].extend(files)

    # Upload statistics
    stats = {
        "total": 0,
        "successful": 0,
        "failed": 0,
        "by_type": {}
    }

    # Upload each file
    for ext, files in files_by_type.items():
        print(f"\nUploading {ext} files: {len(files)}")

        for file_path in files:
            stats["total"] += 1

            try:
                # Calculate relative path for scope
                rel_path = file_path.relative_to(dir_path)
                folder = str(rel_path.parent) if rel_path.parent != Path('.') else "root"

                with open(file_path, "rb") as file:
                    result = trainly.upload_file(
                        file=file,
                        filename=file_path.name,
                        scope_values={
                            "folder": folder,
                            "extension": ext,
                            "full_path": str(rel_path) if preserve_structure else file_path.name
                        }
                    )

                stats["successful"] += 1
                stats["by_type"][ext] = stats["by_type"].get(ext, 0) + 1
                print(f"  ✓ {rel_path}")

            except Exception as e:
                stats["failed"] += 1
                print(f"  ✗ {rel_path}: {e}")

    # Print summary
    print("\n" + "=" * 50)
    print(f"Upload Summary:")
    print(f"  Total: {stats['total']}")
    print(f"  Successful: {stats['successful']}")
    print(f"  Failed: {stats['failed']}")
    print(f"  Success rate: {stats['successful'] / stats['total'] * 100:.1f}%")
    print(f"\nBy type:")
    for ext, count in stats["by_type"].items():
        print(f"  {ext}: {count} files")

    return stats

# Usage
stats = upload_directory(
    "./research_documents",
    preserve_structure=True,
    file_extensions=['.pdf', '.docx', '.txt']
)
```

### Upload with Metadata Extraction

```python
from PyPDF2 import PdfReader
from datetime import datetime
import docx

def extract_pdf_metadata(file_path: str) -> dict:
    """Extract metadata from PDF"""
    try:
        reader = PdfReader(file_path)
        metadata = reader.metadata

        return {
            "title": metadata.get("/Title", "Unknown"),
            "author": metadata.get("/Author", "Unknown"),
            "pages": len(reader.pages),
            "created": metadata.get("/CreationDate", "Unknown")
        }
    except:
        return {}

def extract_docx_metadata(file_path: str) -> dict:
    """Extract metadata from DOCX"""
    try:
        doc = docx.Document(file_path)
        props = doc.core_properties

        return {
            "title": props.title or "Unknown",
            "author": props.author or "Unknown",
            "created": props.created.isoformat() if props.created else "Unknown",
            "modified": props.modified.isoformat() if props.modified else "Unknown",
            "paragraphs": len(doc.paragraphs)
        }
    except:
        return {}

def upload_with_metadata(file_path: str):
    """Upload file and include extracted metadata as scopes"""
    path = Path(file_path)

    # Extract metadata based on file type
    if path.suffix == '.pdf':
        metadata = extract_pdf_metadata(file_path)
    elif path.suffix == '.docx':
        metadata = extract_docx_metadata(file_path)
    else:
        metadata = {}

    # Add file info
    file_stat = path.stat()
    metadata.update({
        "file_size": file_stat.st_size,
        "upload_date": datetime.now().isoformat(),
        "file_extension": path.suffix
    })

    # Upload with metadata
    with open(file_path, "rb") as file:
        result = trainly.upload_file(
            file=file,
            filename=path.name,
            scope_values=metadata
        )

    print(f"Uploaded with metadata: {result.filename}")
    print(f"  Title: {metadata.get('title', 'N/A')}")
    print(f"  Author: {metadata.get('author', 'N/A')}")
    print(f"  Pages: {metadata.get('pages', 'N/A')}")

    return result

# Usage
result = upload_with_metadata("./research_paper.pdf")
```

### Upload from Pandas DataFrame

```python
import pandas as pd
from io import StringIO

def upload_dataframe(
    df: pd.DataFrame,
    name: str,
    description: str = None
):
    """Upload pandas DataFrame as CSV content"""

    # Convert to CSV string
    csv_buffer = StringIO()
    df.to_csv(csv_buffer, index=False)
    csv_content = csv_buffer.getvalue()

    # Add DataFrame info as header
    info_text = f"""
Dataset: {name}
{description or ''}

Rows: {len(df)}
Columns: {len(df.columns)}
Column Names: {', '.join(df.columns)}

Data:
{csv_content}
    """.strip()

    # Upload as text
    result = trainly.upload_text(
        content=info_text,
        name=f"{name}.csv",
        scope_values={
            "type": "dataframe",
            "rows": len(df),
            "columns": len(df.columns),
            "format": "csv"
        }
    )

    print(f"Uploaded DataFrame: {name}")
    print(f"  Shape: {df.shape}")
    print(f"  Columns: {list(df.columns)}")

    return result

# Usage
df = pd.DataFrame({
    'name': ['Alice', 'Bob', 'Charlie'],
    'age': [25, 30, 35],
    'score': [95, 87, 92]
})

result = upload_dataframe(
    df,
    name="student_scores",
    description="Q4 2024 student performance data"
)
```

### Upload from Google Drive

```python
from google.oauth2.credentials import Credentials
from googleapiclient.discovery import build
from googleapiclient.http import MediaIoBaseDownload
from io import BytesIO

def upload_from_google_drive(
    file_id: str,
    filename: str,
    credentials: Credentials
):
    """Download from Google Drive and upload to Trainly"""

    # Build Drive API client
    service = build('drive', 'v3', credentials=credentials)

    # Download file
    request = service.files().get_media(fileId=file_id)
    file_buffer = BytesIO()

    downloader = MediaIoBaseDownload(file_buffer, request)
    done = False

    print("Downloading from Google Drive...")
    while not done:
        status, done = downloader.next_chunk()
        print(f"  Progress: {int(status.progress() * 100)}%")

    # Reset buffer position
    file_buffer.seek(0)

    # Upload to Trainly
    result = trainly.upload_file(
        file=file_buffer,
        filename=filename,
        scope_values={
            "source": "google_drive",
            "drive_file_id": file_id
        }
    )

    print(f"✓ Uploaded from Drive: {result.filename}")
    return result

# Usage (requires google-api-python-client and credentials)
# result = upload_from_google_drive(
#     "1234567890abcdef",
#     "document.pdf",
#     credentials
# )
```

### Upload from S3

```python
import boto3
from io import BytesIO

def upload_from_s3(
    bucket: str,
    key: str,
    filename: str = None
):
    """Download from S3 and upload to Trainly"""

    # Initialize S3 client
    s3 = boto3.client('s3')

    # Download from S3
    buffer = BytesIO()

    print(f"Downloading from S3: s3://{bucket}/{key}")
    s3.download_fileobj(bucket, key, buffer)

    buffer.seek(0)

    # Use key as filename if not provided
    if filename is None:
        filename = key.split('/')[-1]

    # Upload to Trainly
    result = trainly.upload_file(
        file=buffer,
        filename=filename,
        scope_values={
            "source": "s3",
            "bucket": bucket,
            "key": key
        }
    )

    print(f"✓ Uploaded from S3: {result.filename}")
    return result

# Usage
result = upload_from_s3(
    bucket="my-documents",
    key="research/paper.pdf",
    filename="research_paper.pdf"
)
```

### Async Bulk Upload

```python
import asyncio
from pathlib import Path
from trainly import AsyncTrainlyClient

async def async_bulk_upload(file_paths: list):
    """Upload multiple files concurrently"""

    trainly = AsyncTrainlyClient(
        api_key=os.getenv("TRAINLY_API_KEY"),
        chat_id=os.getenv("TRAINLY_CHAT_ID")
    )

    async def upload_one(file_path: str):
        """Upload a single file"""
        try:
            with open(file_path, "rb") as file:
                result = await trainly.upload_file(
                    file=file,
                    filename=Path(file_path).name
                )

            return {"file": Path(file_path).name, "success": True, "result": result}

        except Exception as e:
            return {"file": Path(file_path).name, "success": False, "error": str(e)}

    # Upload all files concurrently
    print(f"Uploading {len(file_paths)} files concurrently...")

    tasks = [upload_one(path) for path in file_paths]
    results = await asyncio.gather(*tasks)

    # Summary
    successful = sum(1 for r in results if r["success"])
    print(f"\n✓ Uploaded {successful}/{len(results)} files")

    return results

# Usage
file_paths = [
    "./doc1.pdf",
    "./doc2.pdf",
    "./doc3.txt"
]

results = asyncio.run(async_bulk_upload(file_paths))

# Print details
for result in results:
    if result["success"]:
        print(f"✓ {result['file']}: {result['result'].file_id}")
    else:
        print(f"✗ {result['file']}: {result['error']}")
```

### Upload with Progress Callback

```python
from pathlib import Path
from typing import Callable

class ProgressTracker:
    """Track upload progress"""

    def __init__(self, total_files: int, callback: Callable = None):
        self.total_files = total_files
        self.completed = 0
        self.failed = 0
        self.callback = callback

    def update(self, success: bool, filename: str, error: str = None):
        """Update progress"""
        if success:
            self.completed += 1
            print(f"✓ [{self.completed}/{self.total_files}] {filename}")
        else:
            self.failed += 1
            print(f"✗ [{self.completed + self.failed}/{self.total_files}] {filename}: {error}")

        # Call custom callback
        if self.callback:
            self.callback(
                completed=self.completed,
                failed=self.failed,
                total=self.total_files,
                filename=filename
            )

    def summary(self):
        """Print summary"""
        print("\n" + "=" * 50)
        print(f"Upload Summary:")
        print(f"  Completed: {self.completed}")
        print(f"  Failed: {self.failed}")
        print(f"  Total: {self.total_files}")
        print(f"  Success rate: {self.completed / self.total_files * 100:.1f}%")

def upload_with_progress(file_paths: list, callback: Callable = None):
    """Upload files with progress tracking"""

    tracker = ProgressTracker(len(file_paths), callback)

    for file_path in file_paths:
        try:
            with open(file_path, "rb") as file:
                result = trainly.upload_file(
                    file=file,
                    filename=Path(file_path).name
                )

            tracker.update(True, Path(file_path).name)

        except Exception as e:
            tracker.update(False, Path(file_path).name, str(e))

    tracker.summary()

# Usage with custom callback
def progress_callback(completed, failed, total, filename):
    """Custom callback for progress updates"""
    progress = (completed + failed) / total * 100
    print(f"  Overall progress: {progress:.1f}%")

upload_with_progress(
    ["./doc1.pdf", "./doc2.pdf", "./doc3.txt"],
    callback=progress_callback
)
```

### Upload JSON Data

```python
import json
from typing import Dict, List

def upload_json_data(
    data: Dict or List,
    name: str,
    pretty: bool = True
):
    """Upload JSON data as a document"""

    # Convert to formatted JSON string
    if pretty:
        json_content = json.dumps(data, indent=2, ensure_ascii=False)
    else:
        json_content = json.dumps(data, ensure_ascii=False)

    # Add metadata header
    content = f"""
JSON Document: {name}

Records: {len(data) if isinstance(data, list) else 'N/A'}
Type: {'Array' if isinstance(data, list) else 'Object'}

Content:
{json_content}
    """.strip()

    # Upload as text
    result = trainly.upload_text(
        content=content,
        name=f"{name}.json",
        scope_values={
            "type": "json",
            "format": "json",
            "pretty": pretty,
            "records": len(data) if isinstance(data, list) else 0
        }
    )

    print(f"Uploaded JSON: {name}")
    return result

# Usage with dictionary
config = {
    "api_version": "1.0",
    "features": ["chat", "upload", "search"],
    "settings": {
        "model": "gpt-4o-mini",
        "temperature": 0.7
    }
}

result = upload_json_data(config, "api_configuration")

# Usage with list
records = [
    {"id": 1, "name": "Alice", "score": 95},
    {"id": 2, "name": "Bob", "score": 87},
    {"id": 3, "name": "Charlie", "score": 92}
]

result = upload_json_data(records, "student_scores")
```

### Upload Web Content

```python
import requests
from bs4 import BeautifulSoup

def upload_webpage(
    url: str,
    extract_text_only: bool = True
):
    """Scrape and upload webpage content"""

    # Fetch webpage
    response = requests.get(url, timeout=10)
    response.raise_for_status()

    if extract_text_only:
        # Extract text content only
        soup = BeautifulSoup(response.content, 'html.parser')

        # Remove script and style elements
        for script in soup(["script", "style"]):
            script.decompose()

        # Get text
        text = soup.get_text()

        # Clean up whitespace
        lines = (line.strip() for line in text.splitlines())
        chunks = (phrase.strip() for line in lines for phrase in line.split("  "))
        text = '\n'.join(chunk for chunk in chunks if chunk)

        content = f"""
Webpage: {url}
Title: {soup.title.string if soup.title else 'No title'}
Extracted: {datetime.now().isoformat()}

Content:
{text}
        """.strip()
    else:
        # Use raw HTML
        content = response.text

    # Upload to Trainly
    result = trainly.upload_text(
        content=content,
        name=f"webpage_{url.split('/')[-1]}.txt",
        scope_values={
            "type": "webpage",
            "url": url,
            "extracted_text": extract_text_only,
            "scraped_at": datetime.now().isoformat()
        }
    )

    print(f"Uploaded webpage: {url}")
    return result

# Usage
result = upload_webpage(
    "https://example.com/article",
    extract_text_only=True
)
```

### Upload with Python Type Checking

```python
from typing import Union, BinaryIO
from pathlib import Path

def type_safe_upload(
    file: Union[str, Path, bytes, BinaryIO],
    filename: str = None,
    **kwargs
):
    """Type-safe upload that handles multiple input types"""

    if isinstance(file, (str, Path)):
        # File path provided
        path = Path(file)
        with open(path, "rb") as f:
            return trainly.upload_file(
                file=f,
                filename=filename or path.name,
                **kwargs
            )

    elif isinstance(file, bytes):
        # Bytes provided
        if not filename:
            raise ValueError("filename required when uploading bytes")

        return trainly.upload_file(
            file=file,
            filename=filename,
            **kwargs
        )

    elif hasattr(file, 'read'):
        # File-like object
        if not filename:
            filename = getattr(file, 'name', 'uploaded_file.txt')

        return trainly.upload_file(
            file=file,
            filename=filename,
            **kwargs
        )

    else:
        raise TypeError(f"Unsupported file type: {type(file)}")

# Usage - all valid
result1 = type_safe_upload("./doc.pdf")  # File path
result2 = type_safe_upload(Path("./doc.pdf"))  # Path object
result3 = type_safe_upload(b"content", filename="data.txt")  # Bytes
result4 = type_safe_upload(open("./doc.pdf", "rb"))  # File object
```

### Upload Using Context Manager

```python
from contextlib import contextmanager
from pathlib import Path
import tempfile
import os

@contextmanager
def temporary_upload(content: str, suffix: str = ".txt"):
    """Context manager for temporary file uploads"""

    # Create temporary file
    fd, temp_path = tempfile.mkstemp(suffix=suffix)

    try:
        # Write content
        with os.fdopen(fd, 'w') as f:
            f.write(content)

        # Upload to Trainly
        with open(temp_path, "rb") as f:
            result = trainly.upload_file(
                file=f,
                filename=Path(temp_path).name,
                scope_values={"temporary": True}
            )

        yield result

    finally:
        # Cleanup
        if os.path.exists(temp_path):
            os.unlink(temp_path)
        print(f"Cleaned up temporary file")

# Usage
with temporary_upload("This is temporary content", suffix=".txt") as result:
    print(f"Uploaded temporarily: {result.file_id}")
    # Do something with the uploaded file
    # File will be cleaned up automatically after the block
```

### List Files

```python
# Get all files in chat
files = trainly.list_files()

print(f"Total files: {files.total_files}")
print(f"Total size: {format_bytes(files.total_size_bytes)}")

for file in files.files:
    print(f"""
    File: {file.filename}
    ID: {file.file_id}
    Size: {format_bytes(file.size_bytes)}
    Chunks: {file.chunk_count}
    Uploaded: {file.upload_date}
    """)

# Helper function
def format_bytes(bytes: int) -> str:
    if bytes == 0:
        return "0 B"

    sizes = ["B", "KB", "MB", "GB", "TB"]
    i = 0
    while bytes >= 1024 and i < len(sizes) - 1:
        bytes /= 1024
        i += 1

    return f"{bytes:.1f} {sizes[i]}"
```

### Advanced File Filtering

```python
from datetime import datetime, timedelta

def list_files_with_filters(
    min_size: int = None,
    max_size: int = None,
    extensions: list = None,
    uploaded_after: datetime = None
):
    """List files with custom filtering"""

    files = trainly.list_files()
    filtered = files.files

    # Filter by size
    if min_size:
        filtered = [f for f in filtered if f.size_bytes >= min_size]

    if max_size:
        filtered = [f for f in filtered if f.size_bytes <= max_size]

    # Filter by extension
    if extensions:
        filtered = [
            f for f in filtered
            if any(f.filename.endswith(ext) for ext in extensions)
        ]

    # Filter by upload date
    if uploaded_after:
        filtered = [
            f for f in filtered
            if datetime.fromisoformat(f.upload_date) > uploaded_after
        ]

    return filtered

# Usage examples

# Get large PDFs only
large_pdfs = list_files_with_filters(
    min_size=1024 * 1024,  # > 1MB
    extensions=['.pdf']
)

# Get recent uploads (last 7 days)
recent_files = list_files_with_filters(
    uploaded_after=datetime.now() - timedelta(days=7)
)

# Get small text files
small_texts = list_files_with_filters(
    max_size=100 * 1024,  # < 100KB
    extensions=['.txt', '.md']
)

print(f"Found {len(large_pdfs)} large PDFs")
print(f"Found {len(recent_files)} recent files")
print(f"Found {len(small_texts)} small text files")
```

### Delete Files

```python
# Delete a specific file
result = trainly.delete_file("v1_user_xyz_doc_123")

print(f"Deleted: {result.filename}")
print(f"Freed: {format_bytes(result.size_bytes_freed)}")
print(f"Chunks removed: {result.chunks_deleted}")

# Delete multiple files
file_ids = ["file_1", "file_2", "file_3"]

for file_id in file_ids:
    try:
        result = trainly.delete_file(file_id)
        print(f"✓ Deleted {result.filename}")
    except Exception as e:
        print(f"✗ Failed to delete {file_id}: {e}")

    # Rate limiting delay
    import time
    time.sleep(0.1)
```

## V1 OAuth Authentication

### Setup V1 Client

```python
from trainly import TrainlyV1Client

# User authenticates with your OAuth provider (Clerk, Auth0, etc.)
user_oauth_token = get_oauth_token()  # Your implementation

# Initialize V1 client
trainly = TrainlyV1Client(
    user_token=user_oauth_token,
    app_id="app_your_app_id"
)

# Query user's private data
response = trainly.query(
    messages=[
        {"role": "user", "content": "What documents do I have?"}
    ],
    response_tokens=200
)

print("Answer:", response.answer)
print("Privacy guarantee:", response.privacy_guarantee)
```

### V1 File Operations

```python
# Upload file to user's private subchat
with open("user_document.pdf", "rb") as file:
    upload_result = trainly.upload_file(
        file=file,
        scope_values={
            "playlist_id": "playlist_123",
            "genre": "rock"
        }
    )

print("Uploaded to user subchat:", upload_result.chat_id)

# List user's files
files = trainly.list_files()
for file in files.files:
    print(f"{file.filename} - {format_bytes(file.size_bytes)}")

# Delete user's file
result = trainly.delete_file(file_id)
print(f"Deleted: {result.filename}")
```

## Python-Specific Patterns

### Decorators for Common Tasks

```python
from functools import wraps
from time import time
import logging

def log_query(func):
    """Decorator to log query execution"""
    @wraps(func)
    def wrapper(*args, **kwargs):
        logger = logging.getLogger(__name__)
        question = kwargs.get('question', args[0] if args else 'unknown')

        logger.info(f"Query started: {question[:50]}...")
        start = time()

        try:
            result = func(*args, **kwargs)
            duration = (time() - start) * 1000
            logger.info(f"Query completed in {duration:.0f}ms")
            return result
        except Exception as e:
            duration = (time() - start) * 1000
            logger.error(f"Query failed after {duration:.0f}ms: {e}")
            raise

    return wrapper

def cache_query(ttl_seconds: int = 300):
    """Decorator to cache query results"""
    cache = {}

    def decorator(func):
        @wraps(func)
        def wrapper(*args, **kwargs):
            # Create cache key
            question = kwargs.get('question', args[0] if args else '')
            cache_key = f"{question}:{kwargs.get('model', 'default')}"

            # Check cache
            if cache_key in cache:
                result, timestamp = cache[cache_key]
                if time() - timestamp < ttl_seconds:
                    print(f"Cache hit! (age: {time() - timestamp:.1f}s)")
                    return result

            # Execute function
            result = func(*args, **kwargs)

            # Store in cache
            cache[cache_key] = (result, time())

            return result

        return wrapper
    return decorator

def measure_credits(func):
    """Decorator to measure credit usage"""
    @wraps(func)
    def wrapper(*args, **kwargs):
        # Get model from kwargs
        model = kwargs.get('model', 'gpt-4o-mini')

        # Execute query
        result = func(*args, **kwargs)

        # Calculate credits
        if hasattr(result, 'usage'):
            tokens = result.usage.total_tokens
            multiplier = MODEL_MULTIPLIERS.get(model, 1)
            credits = (tokens / 1000) * multiplier

            print(f"Credits used: {credits:.3f} ({tokens} tokens, {model})")

        return result

    return wrapper

# Usage
@log_query
@cache_query(ttl_seconds=300)
@measure_credits
def smart_query(question: str, **kwargs):
    return trainly.query(question=question, **kwargs)

# All decorators are applied automatically
response = smart_query("What is AI?", model="gpt-4o-mini")
```

### Property-Based Testing

```python
from hypothesis import given, strategies as st
from trainly import TrainlyClient

trainly = TrainlyClient(
    api_key=os.getenv("TRAINLY_TEST_API_KEY"),
    chat_id=os.getenv("TRAINLY_TEST_CHAT_ID")
)

@given(st.text(min_size=1, max_size=100))
def test_query_handles_any_string(question):
    """Property test: query should handle any valid string"""
    try:
        response = trainly.query(question=question)

        # Properties that should always be true
        assert isinstance(response.answer, str)
        assert len(response.answer) > 0
        assert isinstance(response.context, list)
        assert response.usage.total_tokens > 0

    except Exception as e:
        # Some strings might be rejected (empty, too long, etc.)
        # But the error should be a known type
        assert isinstance(e, (ValidationError, ValueError))

# Run the test with 100 random strings
test_query_handles_any_string()
```

### Custom Context Manager

```python
from contextlib import contextmanager
from trainly import TrainlyClient
import time

class TrainlySession:
    """Session manager with automatic cleanup and stats"""

    def __init__(self, api_key: str, chat_id: str):
        self.client = TrainlyClient(api_key=api_key, chat_id=chat_id)
        self.queries_made = 0
        self.credits_used = 0
        self.start_time = None

    def __enter__(self):
        self.start_time = time.time()
        print(f"Starting Trainly session...")
        return self

    def __exit__(self, exc_type, exc_val, exc_tb):
        duration = time.time() - self.start_time
        print(f"\nSession Summary:")
        print(f"  Duration: {duration:.1f}s")
        print(f"  Queries: {self.queries_made}")
        print(f"  Credits used: {self.credits_used:.3f}")
        if self.queries_made > 0:
            print(f"  Avg credits/query: {self.credits_used / self.queries_made:.3f}")

        return False  # Don't suppress exceptions

    def query(self, question: str, **kwargs):
        """Query with automatic tracking"""
        response = self.client.query(question=question, **kwargs)

        # Track usage
        self.queries_made += 1

        model = kwargs.get('model', 'gpt-4o-mini')
        multiplier = MODEL_MULTIPLIERS.get(model, 1)
        credits = (response.usage.total_tokens / 1000) * multiplier
        self.credits_used += credits

        return response

# Usage
with TrainlySession(
    api_key=os.getenv("TRAINLY_API_KEY"),
    chat_id=os.getenv("TRAINLY_CHAT_ID")
) as session:
    response1 = session.query("What is AI?")
    response2 = session.query("What is ML?")
    response3 = session.query("What is NLP?")
    # Stats printed automatically on exit
```

### Generator-Based File Processing

```python
from pathlib import Path
from typing import Generator, Dict

def process_files_generator(
    directory: str,
    chunk_size: int = 10
) -> Generator[Dict, None, None]:
    """Generator that yields upload results in chunks"""

    files = list(Path(directory).rglob("*.pdf"))

    for i in range(0, len(files), chunk_size):
        chunk = files[i:i + chunk_size]
        results = []

        for file_path in chunk:
            try:
                with open(file_path, "rb") as file:
                    result = trainly.upload_file(
                        file=file,
                        filename=file_path.name
                    )

                results.append({
                    "filename": file_path.name,
                    "success": True,
                    "file_id": result.file_id
                })

            except Exception as e:
                results.append({
                    "filename": file_path.name,
                    "success": False,
                    "error": str(e)
                })

        # Yield chunk results
        yield {
            "chunk": i // chunk_size + 1,
            "total_chunks": (len(files) + chunk_size - 1) // chunk_size,
            "results": results
        }

# Usage
for chunk_result in process_files_generator("./documents", chunk_size=5):
    print(f"\nProcessed chunk {chunk_result['chunk']}/{chunk_result['total_chunks']}")

    successful = sum(1 for r in chunk_result['results'] if r['success'])
    print(f"  Success: {successful}/{len(chunk_result['results'])}")

    # You can pause, save state, or perform other operations between chunks
```

### Dataclass Integration

```python
from dataclasses import dataclass, asdict
from typing import List, Optional
from datetime import datetime

@dataclass
class DocumentMetadata:
    """Structured metadata for documents"""
    title: str
    author: str
    category: str
    tags: List[str]
    created_date: datetime
    language: str = "en"
    version: Optional[str] = None

@dataclass
class UploadJob:
    """Represents a complete upload job"""
    file_path: str
    metadata: DocumentMetadata
    priority: int = 0

    def upload(self, trainly_client: TrainlyClient):
        """Execute the upload"""
        with open(self.file_path, "rb") as file:
            return trainly_client.upload_file(
                file=file,
                filename=Path(self.file_path).name,
                scope_values=asdict(self.metadata)  # Convert dataclass to dict
            )

# Usage
metadata = DocumentMetadata(
    title="Research Paper 2024",
    author="Dr. Smith",
    category="research",
    tags=["ai", "machine-learning", "nlp"],
    created_date=datetime.now(),
    version="1.0"
)

job = UploadJob(
    file_path="./research.pdf",
    metadata=metadata,
    priority=1
)

result = job.upload(trainly)
print(f"Uploaded: {result.filename}")
```

### Multiprocessing for Large Uploads

```python
from multiprocessing import Pool, Manager
from pathlib import Path
import os

def upload_worker(args):
    """Worker function for multiprocessing"""
    file_path, api_key, chat_id, shared_list = args

    # Create client in worker process
    from trainly import TrainlyClient

    trainly = TrainlyClient(
        api_key=api_key,
        chat_id=chat_id
    )

    try:
        with open(file_path, "rb") as file:
            result = trainly.upload_file(
                file=file,
                filename=Path(file_path).name
            )

        result_dict = {
            "filename": Path(file_path).name,
            "success": True,
            "file_id": result.file_id,
            "size": result.size_bytes
        }

        shared_list.append(result_dict)
        return result_dict

    except Exception as e:
        error_dict = {
            "filename": Path(file_path).name,
            "success": False,
            "error": str(e)
        }

        shared_list.append(error_dict)
        return error_dict

def parallel_upload(
    file_paths: list,
    api_key: str,
    chat_id: str,
    workers: int = 4
):
    """Upload files in parallel using multiprocessing"""

    manager = Manager()
    shared_list = manager.list()

    # Prepare arguments for workers
    args_list = [
        (path, api_key, chat_id, shared_list)
        for path in file_paths
    ]

    print(f"Uploading {len(file_paths)} files using {workers} workers...")

    with Pool(processes=workers) as pool:
        results = pool.map(upload_worker, args_list)

    # Summary
    successful = sum(1 for r in results if r["success"])
    total_size = sum(r.get("size", 0) for r in results if r["success"])

    print(f"\nParallel Upload Complete:")
    print(f"  Successful: {successful}/{len(file_paths)}")
    print(f"  Total size: {format_bytes(total_size)}")

    return results

# Usage
file_paths = [f"./doc{i}.pdf" for i in range(1, 21)]

results = parallel_upload(
    file_paths,
    api_key=os.getenv("TRAINLY_API_KEY"),
    chat_id=os.getenv("TRAINLY_CHAT_ID"),
    workers=4
)
```

### Pydantic Integration

```python
from pydantic import BaseModel, Field, validator
from typing import List, Optional
from datetime import datetime

class DocumentUpload(BaseModel):
    """Pydantic model for validated uploads"""

    file_path: str = Field(..., description="Path to file")
    title: str = Field(..., min_length=1, max_length=200)
    category: str = Field(..., description="Document category")
    tags: List[str] = Field(default_factory=list)
    priority: int = Field(default=0, ge=0, le=10)
    author: Optional[str] = None

    @validator('file_path')
    def file_must_exist(cls, v):
        """Validate file exists"""
        if not Path(v).exists():
            raise ValueError(f"File not found: {v}")
        return v

    @validator('file_path')
    def file_size_check(cls, v):
        """Check file size"""
        size = Path(v).stat().st_size
        max_size = 5 * 1024 * 1024  # 5MB

        if size > max_size:
            raise ValueError(f"File too large: {size / 1024 / 1024:.1f}MB (max 5MB)")

        return v

    def upload(self, trainly_client: TrainlyClient):
        """Execute validated upload"""
        with open(self.file_path, "rb") as file:
            return trainly_client.upload_file(
                file=file,
                filename=Path(self.file_path).name,
                scope_values={
                    "title": self.title,
                    "category": self.category,
                    "tags": ",".join(self.tags),
                    "priority": self.priority,
                    "author": self.author or "unknown"
                }
            )

# Usage with validation
try:
    upload = DocumentUpload(
        file_path="./research.pdf",
        title="AI Research 2024",
        category="research",
        tags=["ai", "ml", "nlp"],
        priority=8,
        author="Dr. Smith"
    )

    result = upload.upload(trainly)
    print(f"✓ Uploaded: {result.filename}")

except ValidationError as e:
    print(f"Validation error: {e}")
```

### Query Builder Pattern

```python
from dataclasses import dataclass, field
from typing import Dict, Optional

@dataclass
class QueryBuilder:
    """Fluent API for building queries"""

    _question: str = ""
    _model: str = "gpt-4o-mini"
    _temperature: float = 0.7
    _max_tokens: int = 1000
    _custom_prompt: Optional[str] = None
    _scope_filters: Dict = field(default_factory=dict)

    def ask(self, question: str) -> 'QueryBuilder':
        """Set the question"""
        self._question = question
        return self

    def with_model(self, model: str) -> 'QueryBuilder':
        """Set the AI model"""
        self._model = model
        return self

    def with_temperature(self, temperature: float) -> 'QueryBuilder':
        """Set temperature"""
        self._temperature = temperature
        return self

    def with_max_tokens(self, max_tokens: int) -> 'QueryBuilder':
        """Set max tokens"""
        self._max_tokens = max_tokens
        return self

    def with_prompt(self, prompt: str) -> 'QueryBuilder':
        """Set custom prompt"""
        self._custom_prompt = prompt
        return self

    def filter_by(self, **filters) -> 'QueryBuilder':
        """Add scope filters"""
        self._scope_filters.update(filters)
        return self

    def execute(self, trainly_client: TrainlyClient):
        """Execute the query"""
        if not self._question:
            raise ValueError("Question is required")

        return trainly_client.query(
            question=self._question,
            model=self._model,
            temperature=self._temperature,
            max_tokens=self._max_tokens,
            custom_prompt=self._custom_prompt,
            scope_filters=self._scope_filters if self._scope_filters else None
        )

# Usage with fluent API
response = (QueryBuilder()
    .ask("What is the methodology?")
    .with_model("gpt-4o")
    .with_temperature(0.3)
    .with_max_tokens(2000)
    .filter_by(category="research", version="2.0")
    .execute(trainly))

print(response.answer)
```

### Batch Processing with Queue

```python
from queue import Queue
from threading import Thread, Event
from typing import Callable
import time

class TrainlyBatchProcessor:
    """Process queries in batches with worker threads"""

    def __init__(
        self,
        trainly_client: TrainlyClient,
        num_workers: int = 3,
        batch_delay: float = 0.5
    ):
        self.trainly = trainly_client
        self.num_workers = num_workers
        self.batch_delay = batch_delay

        self.queue = Queue()
        self.results = {}
        self.stop_event = Event()
        self.workers = []

        self._start_workers()

    def _start_workers(self):
        """Start worker threads"""
        for i in range(self.num_workers):
            worker = Thread(target=self._worker, args=(i,), daemon=True)
            worker.start()
            self.workers.append(worker)

    def _worker(self, worker_id: int):
        """Worker thread function"""
        while not self.stop_event.is_set():
            try:
                # Get task from queue (with timeout)
                task = self.queue.get(timeout=1)

                if task is None:
                    break

                task_id, question, callback = task

                # Execute query
                try:
                    response = self.trainly.query(question=question)
                    self.results[task_id] = {
                        "success": True,
                        "response": response
                    }

                    if callback:
                        callback(response, None)

                except Exception as e:
                    self.results[task_id] = {
                        "success": False,
                        "error": str(e)
                    }

                    if callback:
                        callback(None, e)

                finally:
                    self.queue.task_done()

                    # Rate limiting delay
                    time.sleep(self.batch_delay)

            except Queue.Empty:
                continue

    def submit(
        self,
        question: str,
        callback: Callable = None
    ) -> str:
        """Submit a query for processing"""
        task_id = f"task_{int(time.time() * 1000)}"
        self.queue.put((task_id, question, callback))
        return task_id

    def wait(self):
        """Wait for all tasks to complete"""
        self.queue.join()

    def shutdown(self):
        """Shutdown the processor"""
        self.stop_event.set()

        # Signal workers to stop
        for _ in self.workers:
            self.queue.put(None)

        # Wait for workers
        for worker in self.workers:
            worker.join()

        print(f"Processor shutdown. Processed {len(self.results)} queries.")

# Usage
processor = TrainlyBatchProcessor(
    trainly,
    num_workers=3,
    batch_delay=0.5
)

def handle_result(response, error):
    """Callback for results"""
    if error:
        print(f"Error: {error}")
    else:
        print(f"Answer: {response.answer[:100]}...")

# Submit multiple queries
questions = [
    "What is AI?",
    "What is machine learning?",
    "What is deep learning?",
    "What is NLP?",
    "What is computer vision?"
]

for question in questions:
    processor.submit(question, callback=handle_result)

# Wait for completion
processor.wait()
processor.shutdown()
```

### Protocol Pattern for Extensibility

```python
from typing import Protocol, runtime_checkable
from abc import abstractmethod

@runtime_checkable
class FileUploader(Protocol):
    """Protocol for file upload implementations"""

    @abstractmethod
    def upload(self, file, filename: str, **kwargs):
        """Upload a file"""
        pass

class TrainlyUploader:
    """Trainly implementation of FileUploader"""

    def __init__(self, trainly_client: TrainlyClient):
        self.trainly = trainly_client

    def upload(self, file, filename: str, **kwargs):
        return self.trainly.upload_file(
            file=file,
            filename=filename,
            **kwargs
        )

class MockUploader:
    """Mock uploader for testing"""

    def upload(self, file, filename: str, **kwargs):
        print(f"Mock upload: {filename}")
        return {"filename": filename, "success": True}

def process_documents(
    files: list,
    uploader: FileUploader  # Any class that implements the protocol
):
    """Process documents with any uploader implementation"""

    results = []
    for file_path in files:
        with open(file_path, "rb") as file:
            result = uploader.upload(
                file=file,
                filename=Path(file_path).name
            )
        results.append(result)

    return results

# Usage with Trainly
trainly_uploader = TrainlyUploader(trainly)
results = process_documents(["./doc1.pdf", "./doc2.pdf"], trainly_uploader)

# Usage with mock (for testing)
mock_uploader = MockUploader()
results = process_documents(["./doc1.pdf", "./doc2.pdf"], mock_uploader)
```

## Error Handling

### Basic Error Handling

```python
from trainly import (
    TrainlyClient,
    TrainlyError,
    RateLimitError,
    AuthenticationError,
    ValidationError
)

trainly = TrainlyClient(
    api_key="tk_your_api_key",
    chat_id="chat_abc123"
)

try:
    response = trainly.query(
        question="What is the conclusion?"
    )
    print(response.answer)

except RateLimitError as e:
    print(f"Rate limit exceeded. Retry after: {e.retry_after} seconds")
    # Wait and retry
    import time
    time.sleep(e.retry_after)

except AuthenticationError as e:
    print(f"Authentication failed: {e}")
    # Refresh API key or re-login

except ValidationError as e:
    print(f"Invalid request: {e}")
    print(f"Details: {e.details}")

except TrainlyError as e:
    print(f"Trainly API error: {e}")
    print(f"Status code: {e.status}")

except Exception as e:
    print(f"Unexpected error: {e}")
```

### Retry Decorator

```python
import time
from functools import wraps
from trainly import RateLimitError, TrainlyError

def retry_on_rate_limit(max_attempts=3):
    """Decorator to automatically retry on rate limits"""
    def decorator(func):
        @wraps(func)
        def wrapper(*args, **kwargs):
            for attempt in range(max_attempts):
                try:
                    return func(*args, **kwargs)

                except RateLimitError as e:
                    if attempt < max_attempts - 1:
                        wait_time = e.retry_after or (2 ** attempt)
                        print(f"Rate limited. Waiting {wait_time}s...")
                        time.sleep(wait_time)
                    else:
                        raise

                except TrainlyError as e:
                    if attempt < max_attempts - 1 and e.status >= 500:
                        wait_time = 2 ** attempt
                        print(f"Server error. Retrying in {wait_time}s...")
                        time.sleep(wait_time)
                    else:
                        raise

        return wrapper
    return decorator

# Usage
@retry_on_rate_limit(max_attempts=3)
def query_with_retry(question: str):
    return trainly.query(question=question)

response = query_with_retry("What are the findings?")
```

### Context Manager

```python
from trainly import TrainlyClient

# Use context manager for automatic cleanup
with TrainlyClient(
    api_key="tk_your_api_key",
    chat_id="chat_abc123"
) as trainly:
    response = trainly.query(
        question="What is the result?"
    )
    print(response.answer)

# Connection automatically closed
```

## Advanced Features

### Async Support

```python
import asyncio
from trainly import AsyncTrainlyClient

async def main():
    trainly = AsyncTrainlyClient(
        api_key="tk_your_api_key",
        chat_id="chat_abc123"
    )

    # Async query
    response = await trainly.query(
        question="What are the findings?"
    )

    print(response.answer)

    # Async streaming
    async for chunk in trainly.query_stream(
        question="Summarize the results"
    ):
        if chunk.type == "content":
            print(chunk.data, end="", flush=True)
        elif chunk.type == "end":
            print("\n\nComplete!")

asyncio.run(main())
```

### Concurrent Queries

```python
import asyncio
from trainly import AsyncTrainlyClient

async def batch_queries():
    trainly = AsyncTrainlyClient(
        api_key="tk_your_api_key",
        chat_id="chat_abc123"
    )

    questions = [
        "What is the introduction about?",
        "What is the methodology?",
        "What are the conclusions?",
        "What are the limitations?"
    ]

    # Run queries concurrently
    tasks = [
        trainly.query(question=q)
        for q in questions
    ]

    responses = await asyncio.gather(*tasks)

    for question, response in zip(questions, responses):
        print(f"\nQ: {question}")
        print(f"A: {response.answer[:200]}...")

asyncio.run(batch_queries())
```

### Rate Limiting

```python
from trainly import RateLimiter
import time

# Create rate limiter (60 requests per minute)
limiter = RateLimiter(
    max_requests=60,
    window_seconds=60
)

def rate_limited_query(question: str):
    # Wait if necessary
    limiter.wait_for_slot()

    # Make request
    return trainly.query(question=question)

# Use it
response = rate_limited_query("What is the conclusion?")
```

### Caching

```python
from functools import lru_cache
from trainly import TrainlyClient
import hashlib

trainly = TrainlyClient(
    api_key="tk_your_api_key",
    chat_id="chat_abc123"
)

@lru_cache(maxsize=100)
def cached_query(question: str, model: str = "gpt-4o-mini"):
    """Cache query results"""
    response = trainly.query(
        question=question,
        model=model
    )
    return response

# First call - hits API
response1 = cached_query("What is the conclusion?")

# Second call - returns cached result
response2 = cached_query("What is the conclusion?")

# Different question - hits API
response3 = cached_query("What is the methodology?")
```

### Advanced Caching with TTL

```python
from datetime import datetime, timedelta
from typing import Dict, Tuple, Any

class QueryCache:
    def __init__(self, ttl_seconds: int = 300):
        self.cache: Dict[str, Tuple[Any, datetime]] = {}
        self.ttl = timedelta(seconds=ttl_seconds)

    def get(self, question: str):
        if question in self.cache:
            value, timestamp = self.cache[question]
            if datetime.now() - timestamp < self.ttl:
                return value
            else:
                del self.cache[question]
        return None

    def set(self, question: str, value: Any):
        self.cache[question] = (value, datetime.now())

    def clear(self):
        self.cache.clear()

# Usage
cache = QueryCache(ttl_seconds=300)  # 5 minute TTL

def cached_query(question: str):
    # Check cache
    cached = cache.get(question)
    if cached:
        print("Cache hit!")
        return cached

    # Query API
    response = trainly.query(question=question)

    # Store in cache
    cache.set(question, response)

    return response
```

## Framework Integration

### Django

```python
# settings.py
TRAINLY_API_KEY = os.getenv("TRAINLY_API_KEY")
TRAINLY_CHAT_ID = os.getenv("TRAINLY_CHAT_ID")

# views.py
from django.http import JsonResponse
from django.views.decorators.http import require_http_methods
from trainly import TrainlyClient
from django.conf import settings
import json

trainly = TrainlyClient(
    api_key=settings.TRAINLY_API_KEY,
    chat_id=settings.TRAINLY_CHAT_ID
)

@require_http_methods(["POST"])
def query_view(request):
    try:
        data = json.loads(request.body)
        question = data.get("question")

        if not question:
            return JsonResponse(
                {"error": "Question is required"},
                status=400
            )

        response = trainly.query(question=question)

        return JsonResponse({
            "answer": response.answer,
            "context": [
                {
                    "chunk_id": c.chunk_id,
                    "text": c.chunk_text,
                    "score": c.score
                }
                for c in response.context
            ]
        })

    except Exception as e:
        return JsonResponse(
            {"error": str(e)},
            status=500
        )
```

### Flask

```python
from flask import Flask, request, jsonify
from trainly import TrainlyClient, TrainlyError
import os

app = Flask(__name__)

trainly = TrainlyClient(
    api_key=os.getenv("TRAINLY_API_KEY"),
    chat_id=os.getenv("TRAINLY_CHAT_ID")
)

@app.route("/api/query", methods=["POST"])
def query():
    try:
        data = request.get_json()
        question = data.get("question")

        if not question:
            return jsonify({"error": "Question is required"}), 400

        response = trainly.query(question=question)

        return jsonify({
            "answer": response.answer,
            "context": [
                {
                    "chunk_id": c.chunk_id,
                    "text": c.chunk_text,
                    "score": c.score
                }
                for c in response.context
            ],
            "usage": {
                "total_tokens": response.usage.total_tokens
            }
        })

    except TrainlyError as e:
        return jsonify({"error": str(e)}), e.status or 500
    except Exception as e:
        return jsonify({"error": str(e)}), 500

@app.route("/api/upload", methods=["POST"])
def upload():
    try:
        if "file" not in request.files:
            return jsonify({"error": "No file provided"}), 400

        file = request.files["file"]

        result = trainly.upload_file(
            file=file.read(),
            filename=file.filename
        )

        return jsonify({
            "success": True,
            "file_id": result.file_id,
            "filename": result.filename,
            "size_bytes": result.size_bytes
        })

    except Exception as e:
        return jsonify({"error": str(e)}), 500

if __name__ == "__main__":
    app.run(debug=True)
```

### FastAPI

```python
from fastapi import FastAPI, HTTPException, UploadFile, File
from trainly import TrainlyClient, TrainlyError
from pydantic import BaseModel
import os

app = FastAPI()

trainly = TrainlyClient(
    api_key=os.getenv("TRAINLY_API_KEY"),
    chat_id=os.getenv("TRAINLY_CHAT_ID")
)

class QueryRequest(BaseModel):
    question: str
    model: str = "gpt-4o-mini"
    temperature: float = 0.7

class QueryResponse(BaseModel):
    answer: str
    context: list
    usage: dict

@app.post("/api/query", response_model=QueryResponse)
async def query_endpoint(request: QueryRequest):
    try:
        response = trainly.query(
            question=request.question,
            model=request.model,
            temperature=request.temperature
        )

        return QueryResponse(
            answer=response.answer,
            context=[
                {
                    "chunk_id": c.chunk_id,
                    "text": c.chunk_text,
                    "score": c.score
                }
                for c in response.context
            ],
            usage={
                "total_tokens": response.usage.total_tokens
            }
        )

    except TrainlyError as e:
        raise HTTPException(status_code=e.status or 500, detail=str(e))

@app.post("/api/upload")
async def upload_endpoint(file: UploadFile = File(...)):
    try:
        result = trainly.upload_file(
            file=await file.read(),
            filename=file.filename
        )

        return {
            "success": True,
            "file_id": result.file_id,
            "filename": result.filename,
            "size_bytes": result.size_bytes
        }

    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))
```

## Logging & Debugging

### Enable Debug Mode

```python
import logging
from trainly import TrainlyClient

# Configure logging
logging.basicConfig(
    level=logging.DEBUG,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)

# Enable debug mode
trainly = TrainlyClient(
    api_key="tk_your_api_key",
    chat_id="chat_abc123",
    debug=True
)

# All API calls will be logged
response = trainly.query(question="What is the result?")
```

### Custom Logger

```python
import logging
from trainly import TrainlyClient

# Create custom logger
logger = logging.getLogger("trainly_app")
logger.setLevel(logging.INFO)

# Add file handler
file_handler = logging.FileHandler("trainly.log")
file_handler.setFormatter(
    logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
)
logger.addHandler(file_handler)

# Use custom logger
trainly = TrainlyClient(
    api_key="tk_your_api_key",
    chat_id="chat_abc123",
    logger=logger
)
```

### Request/Response Logging

```python
from trainly import TrainlyClient
import json

class LoggingTrainlyClient(TrainlyClient):
    def query(self, **kwargs):
        # Log request
        print("=" * 50)
        print("REQUEST:")
        print(json.dumps(kwargs, indent=2))

        # Make request
        response = super().query(**kwargs)

        # Log response
        print("\nRESPONSE:")
        print(f"Answer length: {len(response.answer)} chars")
        print(f"Citations: {len(response.context)}")
        print(f"Tokens: {response.usage.total_tokens}")
        print("=" * 50)

        return response

trainly = LoggingTrainlyClient(
    api_key="tk_your_api_key",
    chat_id="chat_abc123"
)
```

## Data Classes & Type Hints

### Full Type Definitions

```python
from trainly import (
    TrainlyClient,
    QueryResponse,
    ChunkScore,
    UploadResponse,
    FileInfo,
    ListFilesResponse,
    DeleteResponse,
    Usage
)
from typing import List, Dict, Optional

# Client
client: TrainlyClient = TrainlyClient(
    api_key="tk_key",
    chat_id="chat_id"
)

# Query Response
response: QueryResponse = client.query(question="What is AI?")
answer: str = response.answer
context: List[ChunkScore] = response.context
chat_id: str = response.chat_id
model: str = response.model
usage: Usage = response.usage

# Chunk Score
chunk: ChunkScore = response.context[0]
chunk_id: str = chunk.chunk_id
chunk_text: str = chunk.chunk_text
score: float = chunk.score

# Upload Response
upload_result: UploadResponse = client.upload_file(file=file_bytes)
success: bool = upload_result.success
filename: str = upload_result.filename
file_id: str = upload_result.file_id

# List Files Response
files_response: ListFilesResponse = client.list_files()
files: List[FileInfo] = files_response.files
total_files: int = files_response.total_files
total_size: int = files_response.total_size_bytes

# File Info
file_info: FileInfo = files_response.files[0]
file_id: str = file_info.file_id
filename: str = file_info.filename
upload_date: str = file_info.upload_date
size_bytes: int = file_info.size_bytes
chunk_count: int = file_info.chunk_count

# Delete Response
delete_result: DeleteResponse = client.delete_file(file_id)
message: str = delete_result.message
chunks_deleted: int = delete_result.chunks_deleted
size_freed: int = delete_result.size_bytes_freed
```

## Real-World Examples

### Document Q&A CLI

```python
#!/usr/bin/env python3
from trainly import TrainlyClient
from rich.console import Console
from rich.markdown import Markdown
from rich.prompt import Prompt
import os

console = Console()
trainly = TrainlyClient(
    api_key=os.getenv("TRAINLY_API_KEY"),
    chat_id=os.getenv("TRAINLY_CHAT_ID")
)

def main():
    console.print("[bold cyan]Trainly Document Q&A[/bold cyan]")
    console.print("Type 'exit' to quit\n")

    while True:
        question = Prompt.ask("\n[yellow]Your question[/yellow]")

        if question.lower() in ["exit", "quit"]:
            break

        try:
            console.print("[dim]Searching documents...[/dim]")

            response = trainly.query(question=question)

            # Display answer as markdown
            console.print("\n[bold green]Answer:[/bold green]")
            md = Markdown(response.answer)
            console.print(md)

            # Display citations
            if response.context:
                console.print(f"\n[dim]Based on {len(response.context)} sources[/dim]")

        except Exception as e:
            console.print(f"[bold red]Error:[/bold red] {e}")

if __name__ == "__main__":
    main()
```

### Batch Processing Script

```python
from trainly import TrainlyClient
from pathlib import Path
import csv
import sys

trainly = TrainlyClient(
    api_key=os.getenv("TRAINLY_API_KEY"),
    chat_id=os.getenv("TRAINLY_CHAT_ID")
)

def batch_process_questions(questions_file: str, output_file: str):
    """Process a CSV of questions and save answers"""

    # Read questions
    with open(questions_file, 'r') as f:
        reader = csv.DictReader(f)
        questions = list(reader)

    # Process each question
    results = []
    for i, row in enumerate(questions, 1):
        print(f"Processing {i}/{len(questions)}: {row['question'][:50]}...")

        try:
            response = trainly.query(
                question=row['question'],
                model=row.get('model', 'gpt-4o-mini')
            )

            results.append({
                "question": row['question'],
                "answer": response.answer,
                "tokens": response.usage.total_tokens,
                "citations": len(response.context),
                "status": "success"
            })

        except Exception as e:
            results.append({
                "question": row['question'],
                "answer": "",
                "tokens": 0,
                "citations": 0,
                "status": f"error: {e}"
            })

    # Save results
    with open(output_file, 'w', newline='') as f:
        writer = csv.DictWriter(f, fieldnames=[
            'question', 'answer', 'tokens', 'citations', 'status'
        ])
        writer.writeheader()
        writer.writerows(results)

    print(f"\nResults saved to {output_file}")

if __name__ == "__main__":
    if len(sys.argv) != 3:
        print("Usage: python batch_process.py questions.csv output.csv")
        sys.exit(1)

    batch_process_questions(sys.argv[1], sys.argv[2])
```

### Jupyter Notebook Integration

```python
# In Jupyter Notebook
from trainly import TrainlyClient
import pandas as pd
from IPython.display import display, Markdown

trainly = TrainlyClient(
    api_key="tk_your_api_key",
    chat_id="chat_abc123"
)

# Query and display as markdown
def ask(question: str):
    response = trainly.query(question=question)

    # Display answer
    display(Markdown(f"### Answer\n{response.answer}"))

    # Display citations as dataframe
    if response.context:
        citations_df = pd.DataFrame([
            {
                "Citation": i,
                "Text": c.chunk_text[:100] + "...",
                "Relevance": f"{c.score * 100:.1f}%"
            }
            for i, c in enumerate(response.context[:5])
        ])
        display(citations_df)

    return response

# Usage
response = ask("What are the main findings?")
```

## Testing

### Unit Tests

```python
import unittest
from unittest.mock import Mock, patch
from trainly import TrainlyClient, QueryResponse

class TestTrainlyIntegration(unittest.TestCase):
    def setUp(self):
        self.trainly = TrainlyClient(
            api_key="tk_test_key",
            chat_id="chat_test_123"
        )

    @patch('trainly.client.requests.post')
    def test_query(self, mock_post):
        # Mock response
        mock_post.return_value.json.return_value = {
            "answer": "Test answer",
            "context": [],
            "chat_id": "chat_test_123",
            "model": "gpt-4o-mini",
            "usage": {
                "prompt_tokens": 10,
                "completion_tokens": 20,
                "total_tokens": 30
            }
        }
        mock_post.return_value.status_code = 200

        # Test query
        response = self.trainly.query(question="Test question")

        self.assertEqual(response.answer, "Test answer")
        self.assertEqual(response.model, "gpt-4o-mini")

    def test_query_validation(self):
        # Test empty question
        with self.assertRaises(ValueError):
            self.trainly.query(question="")

if __name__ == "__main__":
    unittest.main()
```

### Integration Tests

```python
import pytest
from trainly import TrainlyClient
import os

@pytest.fixture
def trainly_client():
    return TrainlyClient(
        api_key=os.getenv("TRAINLY_TEST_API_KEY"),
        chat_id=os.getenv("TRAINLY_TEST_CHAT_ID")
    )

def test_query_basic(trainly_client):
    response = trainly_client.query(
        question="What is 2+2?"
    )

    assert response.answer
    assert response.chat_id
    assert response.usage.total_tokens > 0

def test_query_with_scopes(trainly_client):
    response = trainly_client.query(
        question="Test question",
        scope_filters={"project_id": "test_proj"}
    )

    assert response.answer

@pytest.mark.asyncio
async def test_async_query():
    from trainly import AsyncTrainlyClient

    trainly = AsyncTrainlyClient(
        api_key=os.getenv("TRAINLY_TEST_API_KEY"),
        chat_id=os.getenv("TRAINLY_TEST_CHAT_ID")
    )

    response = await trainly.query(question="Test")

    assert response.answer
```

## Best Practices

<AccordionGroup>
  <Accordion title="Environment Variables">
    Always use environment variables for credentials:

    ```python
    # .env file
    TRAINLY_API_KEY=tk_your_api_key
    TRAINLY_CHAT_ID=chat_abc123

    # Load with python-dotenv
    from dotenv import load_dotenv
    import os

    load_dotenv()

    trainly = TrainlyClient(
        api_key=os.getenv("TRAINLY_API_KEY"),
        chat_id=os.getenv("TRAINLY_CHAT_ID")
    )
    ```

  </Accordion>

  <Accordion title="Connection Pooling">
    Reuse client instances for better performance:

    ```python
    # ✅ GOOD - Reuse client
    trainly = TrainlyClient(api_key="...", chat_id="...")

    for question in questions:
        response = trainly.query(question=question)

    # ❌ BAD - Create new client each time
    for question in questions:
        trainly = TrainlyClient(api_key="...", chat_id="...")
        response = trainly.query(question=question)
    ```

  </Accordion>

  <Accordion title="Async for Performance">
    Use async client for concurrent operations:

    ```python
    import asyncio
    from trainly import AsyncTrainlyClient

    async def process_many_queries(questions):
        trainly = AsyncTrainlyClient(...)

        # Run concurrently
        tasks = [
            trainly.query(question=q)
            for q in questions
        ]

        return await asyncio.gather(*tasks)
    ```

  </Accordion>
</AccordionGroup>

## Troubleshooting

<AccordionGroup>
  <Accordion title="Import Errors">
    ```bash
    ModuleNotFoundError: No module named 'trainly'
    ```

    **Solution**: Install the SDK
    ```bash
    pip install trainly
    ```

  </Accordion>

  <Accordion title="Authentication Errors">
    ```
    AuthenticationError: Invalid API key
    ```

    **Solutions**:
    - Check API key starts with `tk_`
    - Verify API access is enabled in chat settings
    - Ensure chat has published settings

  </Accordion>

  <Accordion title="Empty Responses">
    ```python
    response.answer == "I don't have any context..."
    ```

    **Solutions**:
    - Upload documents to the chat
    - Check published settings include context files
    - Verify scope filters aren't too restrictive

  </Accordion>

  <Accordion title="Rate Limit Errors">
    ```
    RateLimitError: Rate limit exceeded
    ```

    **Solutions**:
    - Implement exponential backoff
    - Use caching for common queries
    - Consider upgrading your plan

  </Accordion>
</AccordionGroup>

## Next Steps

<CardGroup cols={2}>
  <Card title="JavaScript SDK" icon="js" href="/javascript-sdk">
    JavaScript/TypeScript SDK documentation
  </Card>
  <Card title="API Reference" icon="book" href="/api-reference/introduction">
    Complete REST API documentation
  </Card>
  <Card title="Examples" icon="code" href="/examples">
    More integration examples
  </Card>
  <Card title="Support" icon="question" href="/support">
    Get help from our team
  </Card>
</CardGroup>
